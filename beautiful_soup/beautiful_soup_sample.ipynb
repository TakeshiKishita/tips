{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from os import path\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = ''\n",
    "CAT_BASE = 'jp/news/products/'\n",
    "CATEGORY = []\n",
    "TARGET = '/jp/news/story/'\n",
    "\n",
    "OUT_PUT = 'test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "for cat_idx, cat in enumerate(CATEGORY):\n",
    "    # 各カテゴリ内の'jp/news/products/'から始まるリンクのURLを取得\n",
    "    html = urllib.request.urlopen(urljoin(BASE, CAT_BASE+cat+'.html'))\n",
    "    soup = BeautifulSoup(html)\n",
    "    target_tags = soup.select('a[href^=\"'+TARGET+'\"]')\n",
    "\n",
    "    # 取得したリンクを辞書に代入する\n",
    "    for tag in target_tags:\n",
    "        target_url = tag.attrs['href']\n",
    "        if target_url in url_dict:\n",
    "            url_dict[target_url].append(cat_idx)\n",
    "        else:\n",
    "            url_dict[target_url] = [cat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783\n",
      "/jp/news/story/1109275109.html\n"
     ]
    }
   ],
   "source": [
    "print(len(url_dict))\n",
    "print(target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_PUT, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    \n",
    "    for url, categorys in url_dict.items():\n",
    "        # 各URLから記事に関するタグを取得\n",
    "        html = urllib.request.urlopen(urljoin(BASE, url))\n",
    "        soup = BeautifulSoup(html)\n",
    "        news_block_tags = soup.findAll('section', id=re.compile('news_block*'))\n",
    "\n",
    "        # タグの中からテキストのみを抽出\n",
    "        sentence = ''\n",
    "        for i, tag in enumerate(news_block_tags):\n",
    "            text = tag.get_text()\n",
    "\n",
    "            # 不要な文字列を削除\n",
    "            text = re.sub('MHI_MODULES.renderNewsSummary(\".+\")', '', text)\n",
    "            text = re.sub('<.+/>', '', text)\n",
    "            text = re.sub('[\\r,\\n]', ' ', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            sentence += ' '+text if i > 0 else text\n",
    "        sentence =  re.sub('^ *', '', sentence)\n",
    "        sentence =  re.sub(' *$', '', sentence)\n",
    "\n",
    "        # TODO カテゴリの重複処理が必要\n",
    "        writer.writerow([str(categorys[0]), sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
